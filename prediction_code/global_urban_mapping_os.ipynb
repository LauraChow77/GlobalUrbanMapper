{"cells":[{"cell_type":"markdown","metadata":{"id":"92tjHsUaCFiE"},"source":["Code Overview:\n","This script provides a user-friendly approach for researchers without GPU resources to perform urban mapping globally.\n","Before executing this script, ensure you have met the following prerequisites:\n","- Acquired multimodal imagery for your target study area according to the instructions provided at: https://github.com/LauraChow77/GlobalUrbanMapper/tree/main/gee_code.\n","- Obtained the Global Urban Mapper (GUM) model checkpoint and its associated configuration file as per the guidelines detailed at: https://github.com/LauraChow77/GlobalUrbanMapper/tree/main/model.\n","- Configured your Google Colab notebook to use a T4 GPU by going to 'Runtime' -> 'Change runtime type' -> 'Hardware accelerator' and selecting 'T4 GPU'.\n","\n","Assistance:\n","If you encounter any issues or have questions, please do not hesitate to contact me at 22042458r@connect.polyu.hk"]},{"cell_type":"markdown","metadata":{"id":"zjprBZ6ciMYI"},"source":["# Set Up"]},{"cell_type":"markdown","metadata":{"id":"ekkKKuWHrynP"},"source":["Setup Section:\n","This section prepares the environment for the global urban mapping task by:\n","- Installing necessary dependencies.\n","- Importing required libraries.\n","- Authenticating Google Drive access for file retrieval (e.g., multimodal imagery, model files) and for saving model predictions."]},{"cell_type":"markdown","metadata":{"id":"ik8S7C8QvXuH"},"source":["## Install dependencies"]},{"cell_type":"markdown","metadata":{"id":"E2k671nC8I45"},"source":["Please restart the runtime after executing the cell BELOW. Navigate to 'Runtime' in the menu and select 'Restart Runtime'."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rojMVhEekTAz"},"outputs":[],"source":["!pip install rasterio\n","!pip install ftfy\n","!pip3 install openmim\n","!mim install mmengine\n","!mim install \"mmcv>=2.0.0\"\n","!pip install mmsegmentation"]},{"cell_type":"markdown","metadata":{"id":"fdm1HGv58ALm"},"source":["Please restart the runtime after executing the cell ABOVE. Navigate to 'Runtime' in the menu and select 'Restart session'."]},{"cell_type":"markdown","metadata":{"id":"2IXVUlfkvd8y"},"source":["## Import libraries"]},{"cell_type":"markdown","source":["ATTENTION: mmcv just released a new version but the downstream libraries (e.g., mmsegmentation) are not yet supported. You might encounter issues such as:\n","\n","AssertionError: MMCV==2.2.0 is used but incompatible. Please install mmcv>=2.0.0rc4.\n","\n","To handle this issue, open /usr/local/lib/python3.10/dist-packages/mmseg/__init__.py file and comment out the related assert code (l61-l63). (reference: https://github.com/open-mmlab/mmcv/issues/3096)"],"metadata":{"id":"bafKq8kbdo7e"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"tfIBB-rgggnx"},"outputs":[],"source":["import ee\n","from google.colab import auth\n","\n","import os\n","import rasterio\n","import numpy as np\n","from tqdm import tqdm\n","\n","import torch\n","import torch.optim as optim\n","import torch.nn as nn\n","from torchvision import transforms\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","from torch.utils.data import TensorDataset\n","\n","from mmengine.registry import init_default_scope\n","from mmengine import Config\n","from mmseg.apis import inference_model, init_model\n","from mmseg.models import build_segmentor"]},{"cell_type":"markdown","metadata":{"id":"ykXDwSTyvhYA"},"source":["## Mount Google Drive"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9cDFuKxxo5Bb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1715948243055,"user_tz":-480,"elapsed":27119,"user":{"displayName":"周煜晗","userId":"13311205834198010795"}},"outputId":"125d04ca-1215-435c-c3c8-9d1aff9574f8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"]},{"cell_type":"markdown","metadata":{"id":"YPnmeW_MjRDk"},"source":["# Global Variables"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RwuhPvI1jEmR"},"outputs":[],"source":["\"\"\" Model inference variables\"\"\"\n","BATCH_SIZE = 1\n","NUM_WORKERS = 16\n","with_product = False                                                                                        # if True, the model relies on multimodal data to generate maps; if False, then the model will ALSO relies on three products.\n","INPUT_CHANNELS = 13 if with_product else 10\n","DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","MODEL_CONFIG =  '/content/drive/My Drive/GUM/cfg/unet_newest_version.py'                                    # sample for the model config path\n","MODEL_WITH_PRODUCT_CONFIG =  '/content/drive/My Drive/GUM/cfg/unet_with_product_newest_version.py'\n","CKPT_PATH = '/content/drive/My Drive/GUM/model/stage2.pth'                                                  # sample for the model path\n","CKPT_WITH_PRODUCT_PATH = '/content/drive/My Drive/GUM/model/stage2_with_product.pth'\n","INFERENCE_DATASET_DIR = '/content/drive/My Drive/GUM/datasets'                                              # sample the multimodal image(s) directory\n","PREDICTED_DATASET_DIR = '/content/drive/My Drive/GUM/output'                                                # sample for the predicted image directory\n","\n","\n","data_transforms = transforms.Compose([\n","    transforms.ToTensor(),\n","])"]},{"cell_type":"markdown","metadata":{"id":"FlQ2MjLRu6oR"},"source":["# Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TijhDtYNu8sJ"},"outputs":[],"source":["class GUM(Dataset):\n","    def __init__(self, root_dir, with_product=False, transform=None, inference_mode=False):\n","        \"\"\"\n","        Args:\n","            root_dir (string): Directory with all the images.\n","            with_product (bool): Decide if the model will rely on products' information.\n","            transform (callable, optional): Optional transform to be applied on a sample.\n","            inference_mode (bool, optional): Flag to indicate whether the dataset is used for inference.\n","        \"\"\"\n","        self.root_dir = root_dir\n","        self.transform = transform\n","        self.with_product = with_product\n","        self.inference_mode = inference_mode\n","\n","        self.data = []\n","\n","        if not self.inference_mode:  # Training/Validation mode\n","            for cls_name in self.class_names:\n","                cls_folder = os.path.join(root_dir, cls_name)\n","                for img_filename in os.listdir(cls_folder):\n","                    if img_filename.endswith('.tif'):\n","                        img_path = os.path.join(cls_folder, img_filename)\n","                        label_path = img_path.replace('img_stack_with_product', 'label')\n","                        self.data.append((img_path, label_path))\n","        else:  # Inference mode\n","            if os.path.isdir(root_dir):\n","                for img_filename in os.listdir(root_dir):\n","                    if img_filename.endswith('.tif'):\n","                        img_path = os.path.join(root_dir, img_filename)\n","                        self.data.append((img_path, None))\n","            else:\n","                raise FileNotFoundError(f\"The directory {root_dir} does not exist.\")\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        img_path, label = self.data[idx]\n","\n","        with rasterio.open(img_path) as data:\n","            multimodal_data = data.read()\n","\n","        multimodal_data = np.nan_to_num(multimodal_data, nan=0.0, posinf=0.0, neginf=0.0)\n","\n","        # Handle s2 data\n","        multimodal_data[0:4, :, :] = np.nan_to_num(multimodal_data[0:4, :, :], nan=0.0, posinf=0.0, neginf=0.0)\n","        multimodal_data[0:4, :, :] = np.clip(multimodal_data[:4, :, :], 0, 1)\n","\n","        # Handle s1 data\n","        clip_s1_min, clip_s1_max = -25, 0\n","        multimodal_data[4:8, :, :] = np.nan_to_num(multimodal_data[4:8, :, :], nan=-25, posinf=-25, neginf=-25)\n","        s1 = multimodal_data[4:8, :, :]\n","        s1 = np.clip(s1, clip_s1_min, clip_s1_max)\n","        s1 = ((s1 - clip_s1_min)/25).astype(np.float32)\n","        multimodal_data[4:8, :, :] = s1\n","\n","        # Handle dem data as per the provided code snippet\n","        multimodal_data[9:11, :, :] = np.nan_to_num(multimodal_data[9:11, :, :], nan=0.0, posinf=0.0, neginf=0.0)\n","        slope, aspect = multimodal_data[9:10, :, :], multimodal_data[10:11, :, :]\n","        slope = np.clip(slope, 0, 90)\n","        slope = slope / 90\n","        aspect = np.clip(aspect, 0, 360)\n","        aspect = aspect / 360\n","        slope_aspect = np.concatenate((slope, aspect), axis=0)\n","\n","        multimodal_data[8:10, :, :] = slope_aspect\n","\n","        if self.with_product:\n","            multimodal_data[10:13] = multimodal_data[11:14]\n","            img = np.transpose(multimodal_data[:13, :, :], (1, 2, 0))\n","        else:\n","            img = np.transpose(multimodal_data[:10, :, :], (1, 2, 0))\n","\n","        # Apply the transformations\n","        if self.transform:\n","            img = self.transform(img)  # img is a numpy array and will be converted to a tensor here\n","\n","        file_name = os.path.basename(img_path)\n","        results = {}\n","        results['filename'] = file_name\n","        results['ori_filename'] = file_name\n","        results['img'] = img\n","        results['img_shape'] = img.shape\n","        results['ori_shape'] = img.shape\n","        # Set initial values for default meta_keys\n","        results['pad_shape'] = img.shape\n","        results['scale_factor'] = 1.0\n","        num_channels = 1 if len(img.shape) < 3 else img.shape[2]\n","        results['img_norm_cfg'] = dict(\n","            mean=np.zeros(num_channels, dtype=np.float32),\n","            std=np.ones(num_channels, dtype=np.float32),\n","            to_rgb=False)\n","        results['flip'] = False\n","        # For inference, return the image and the file name\n","        if self.inference_mode:\n","            return img, results  # Return the image path instead of the label\n","        else:\n","            return img, label"]},{"cell_type":"markdown","metadata":{"id":"0XDNWiX0nVtD"},"source":["# Main"]},{"cell_type":"markdown","metadata":{"id":"n7PBtKjbxX6L"},"source":["Main Section:\n","This section carries out the global urban mapping task. It involves the following steps:\n","- Loading the necessary multimodal dataset and the pretrained model.\n","- Using the loaded model to predict urban areas within the provided multimodal data."]},{"cell_type":"markdown","metadata":{"id":"XBVCwdIWoK12"},"source":["## Load data and model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VZi1HVxnnXTq"},"outputs":[],"source":["# Prepare the multimodal dataset for prediction.\n","init_default_scope('mmseg')\n","inference_dataset = GUM(INFERENCE_DATASET_DIR, with_product=with_product, transform=data_transforms, inference_mode=True)\n","inference_loader = DataLoader(inference_dataset, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, shuffle=False)\n","\n","# Initialize the Global Urban Mapper model for inference.\n","if not with_product:\n","    model = init_model(MODEL_CONFIG, CKPT_PATH, device=DEVICE)\n","else:\n","    model = init_model(MODEL_WITH_PRODUCT_CONFIG, CKPT_WITH_PRODUCT_PATH, device=DEVICE)"]},{"cell_type":"markdown","metadata":{"id":"yNUfjUvVoIqD"},"source":["## Prediction"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_qKz0X5Ba0zg"},"outputs":[],"source":["model.eval()  # Set the model to evaluation mode\n","with torch.no_grad():\n","    for inputs, img_metas in tqdm(inference_loader, desc='Inference Progress', unit='batch'):\n","\n","        inputs = inputs.to(DEVICE)\n","        outputs = model.test_step([inputs])\n","\n","        file_name = img_metas['filename'][0] # Assuming img_meta is a list of lists of dicts\n","        input_path = os.path.join(INFERENCE_DATASET_DIR, file_name)\n","        output_path = os.path.join(PREDICTED_DATASET_DIR, file_name)\n","        with rasterio.open(input_path) as src:\n","            profile = src.profile\n","        # Adjust the profile to match the output dimensions and data type\n","        output = outputs[0].pred_sem_seg.data.cpu().numpy()\n","        print(output.shape)\n","        profile.update(\n","            dtype=np.uint8,\n","            count=1  # Update the number of bands to the output's number of bands\n","        )\n","        # Save the prediction\n","        if output.ndim == 2:\n","            output = output[np.newaxis, :, :]\n","\n","        # Write the output with the same profile as the input image\n","        with rasterio.open(output_path, 'w', **profile) as dst:\n","            dst.write(output)"]},{"cell_type":"code","source":[],"metadata":{"id":"vuSFz9Jkrhk7"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","toc_visible":true,"provenance":[],"authorship_tag":"ABX9TyMIUDUV0xkuagFQ44eF4C5+"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}