{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyOaZ9z5CLnjxv522h/8XcbR"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Code Overview:\n","This script provides a user-friendly approach for researchers without GPU resources to perform urban mapping globally.\n","Before executing this script, ensure you have met the following prerequisites:\n","- Acquired multimodal imagery for your target study area according to the instructions provided at: https://github.com/LauraChow77/GlobalUrbanMapper/tree/main/gee_code.\n","- Obtained the Global Urban Mapper (GUM) model checkpoint and its associated configuration file as per the guidelines detailed at: https://github.com/LauraChow77/GlobalUrbanMapper/tree/main/model.\n","- Configured your Google Colab notebook to use a T4 GPU by going to 'Runtime' -> 'Change runtime type' -> 'Hardware accelerator' and selecting 'T4 GPU'.\n","\n","Assistance:\n","If you encounter any issues or have questions, please do not hesitate to contact me at 22042458r@connect.polyu.hk"],"metadata":{"id":"92tjHsUaCFiE"}},{"cell_type":"markdown","source":["# Set Up"],"metadata":{"id":"zjprBZ6ciMYI"}},{"cell_type":"markdown","source":["Setup Section:\n","This section prepares the environment for the global urban mapping task by:\n","- Installing necessary dependencies.\n","- Importing required libraries.\n","- Authenticating Google Drive access for file retrieval (e.g., multimodal imagery, model files) and for saving model predictions."],"metadata":{"id":"ekkKKuWHrynP"}},{"cell_type":"markdown","source":["## Install dependencies"],"metadata":{"id":"ik8S7C8QvXuH"}},{"cell_type":"markdown","source":["Please restart the runtime after executing the cell BELOW. Navigate to 'Runtime' in the menu and select 'Restart Runtime'."],"metadata":{"id":"E2k671nC8I45"}},{"cell_type":"code","source":["!pip install rasterio\n","!pip install ftfy\n","!pip3 install openmim\n","!mim install mmengine\n","!mim install \"mmcv>=2.0.0\"\n","!pip install mmsegmentation"],"metadata":{"id":"rojMVhEekTAz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Please restart the runtime after executing the cell ABOVE. Navigate to 'Runtime' in the menu and select 'Restart Runtime'."],"metadata":{"id":"fdm1HGv58ALm"}},{"cell_type":"markdown","source":["## Import libraries"],"metadata":{"id":"2IXVUlfkvd8y"}},{"cell_type":"code","execution_count":3,"metadata":{"id":"tfIBB-rgggnx","executionInfo":{"status":"ok","timestamp":1709645826719,"user_tz":-480,"elapsed":10883,"user":{"displayName":"周煜晗","userId":"13311205834198010795"}}},"outputs":[],"source":["import ee\n","from google.colab import auth\n","\n","import os\n","import rasterio\n","import numpy as np\n","from tqdm import tqdm\n","\n","import torch\n","import torch.optim as optim\n","import torch.nn as nn\n","from torchvision import transforms\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","from torch.utils.data import TensorDataset\n","\n","from mmengine.registry import init_default_scope\n","from mmengine import Config\n","from mmseg.apis import inference_model, init_model\n","from mmseg.models import build_segmentor"]},{"cell_type":"markdown","source":["## Mount Google Drive"],"metadata":{"id":"ykXDwSTyvhYA"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"metadata":{"id":"9cDFuKxxo5Bb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1709645748860,"user_tz":-480,"elapsed":3417,"user":{"displayName":"周煜晗","userId":"13311205834198010795"}},"outputId":"01313a1c-aefe-44b3-c534-906ac39df73b"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["# Global Variables"],"metadata":{"id":"YPnmeW_MjRDk"}},{"cell_type":"code","source":["# Model Inference Configuration:\n","# These variables are set for performing inference with the model.\n","\n","BATCH_SIZE = 1\n","NUM_WORKERS = 16\n","INPUT_CHANNELS = 10\n","DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","MODEL_CONFIG =  'INSERT_PATH_TO_GUM_CHECKPOINT_CONFIG_HERE'\n","CKPT_PATH = 'INSERT_PATH_TO_GUM_CHECKPOINT_HERE'\n","INFERENCE_DATASET_DIR = 'INSERT_PATH_TO_INFERENCE_DATASET_HERE'\n","PREDICTED_DATASET_DIR = 'INSERT_PATH_TO_PREDICTED_RESULTS_HERE'\n","\n","data_transforms = transforms.Compose([\n","    transforms.ToTensor(),\n","])"],"metadata":{"id":"RwuhPvI1jEmR","executionInfo":{"status":"ok","timestamp":1709645826719,"user_tz":-480,"elapsed":3,"user":{"displayName":"周煜晗","userId":"13311205834198010795"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["# Dataset"],"metadata":{"id":"FlQ2MjLRu6oR"}},{"cell_type":"code","source":["class GUM(Dataset):\n","    def __init__(self, root_dir, transform=None, inference_mode=False):\n","        \"\"\"\n","        Args:\n","            root_dir (string): Directory with all the images.\n","            transform (callable, optional): Optional transform to be applied on a sample.\n","            inference_mode (bool, optional): Flag to indicate whether the dataset is used for inference.\n","        \"\"\"\n","        self.root_dir = root_dir\n","        self.transform = transform\n","        self.inference_mode = inference_mode\n","\n","        self.data = []\n","\n","        if not self.inference_mode:  # Training/Validation mode\n","            for cls_name in self.class_names:\n","                cls_folder = os.path.join(root_dir, cls_name)\n","                for img_filename in os.listdir(cls_folder):\n","                    if img_filename.endswith('.tif'):\n","                        img_path = os.path.join(cls_folder, img_filename)\n","                        label_path = img_path.replace('img_stack_with_product', 'label')\n","                        self.data.append((img_path, label_path))\n","        else:  # Inference mode\n","            if os.path.isdir(root_dir):\n","                for img_filename in os.listdir(root_dir):\n","                    if img_filename.endswith('.tif'):\n","                        img_path = os.path.join(root_dir, img_filename)\n","                        self.data.append((img_path, None))\n","            else:\n","                raise FileNotFoundError(f\"The directory {root_dir} does not exist.\")\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        img_path, label = self.data[idx]\n","\n","        with rasterio.open(img_path) as data:\n","            multimodal_data = data.read()\n","\n","        multimodal_data = np.nan_to_num(multimodal_data)\n","\n","        # Handle s1 data\n","        s1 = multimodal_data[4:8, :, :]\n","        clip_s1_min, clip_s1_max = -25, 0\n","        s1 = np.clip(s1, clip_s1_min, clip_s1_max)\n","        s1 = (s1 - clip_s1_min)/25\n","        s1 = s1.astype(np.float32)\n","        multimodal_data[4:8, :, :] = s1\n","\n","        # Handle dem data\n","        slope, aspect = multimodal_data[9:10, :, :], multimodal_data[10:11, :, :]\n","        slope = np.clip(slope, 0, 90)\n","        slope = slope / 90\n","        aspect = np.clip(aspect, 0, 360)\n","        aspect = aspect / 360\n","        slope_aspect = np.concatenate((slope, aspect), axis=0)\n","        # Replace the corresponding slices in multimodal_data with the modified slope and aspect\n","        multimodal_data[8:10, :, :] = slope_aspect\n","\n","        img = multimodal_data[:10, :, :]\n","\n","        # Manually transpose the array from (C, H, W) to (H, W, C)\n","        img = np.transpose(multimodal_data[:10, :, :], (1, 2, 0))\n","\n","        # Apply the transformations\n","        if self.transform:\n","            img = self.transform(img)  # img is a numpy array and will be converted to a tensor here\n","\n","        file_name = os.path.basename(img_path)\n","        results = {}\n","        results['filename'] = file_name\n","        results['ori_filename'] = file_name\n","        results['img'] = img\n","        results['img_shape'] = img.shape\n","        results['ori_shape'] = img.shape\n","        # Set initial values for default meta_keys\n","        results['pad_shape'] = img.shape\n","        results['scale_factor'] = 1.0\n","        num_channels = 1 if len(img.shape) < 3 else img.shape[2]\n","        results['img_norm_cfg'] = dict(\n","            mean=np.zeros(num_channels, dtype=np.float32),\n","            std=np.ones(num_channels, dtype=np.float32),\n","            to_rgb=False)\n","        results['flip'] = False\n","        # For inference, return the image and the file name\n","        if self.inference_mode:\n","\n","            return img, results  # Return the image path instead of the label\n","        else:\n","            return img, label\n","\n","\n","        return img, label"],"metadata":{"id":"TijhDtYNu8sJ","executionInfo":{"status":"ok","timestamp":1709645826719,"user_tz":-480,"elapsed":2,"user":{"displayName":"周煜晗","userId":"13311205834198010795"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["# Main"],"metadata":{"id":"0XDNWiX0nVtD"}},{"cell_type":"markdown","source":["Main Section:\n","This section carries out the global urban mapping task. It involves the following steps:\n","- Loading the necessary multimodal dataset and the pretrained model.\n","- Using the loaded model to predict urban areas within the provided multimodal data."],"metadata":{"id":"n7PBtKjbxX6L"}},{"cell_type":"markdown","source":["## Load data and model"],"metadata":{"id":"XBVCwdIWoK12"}},{"cell_type":"code","source":["# Prepare the multimodal dataset for prediction.\n","init_default_scope('mmseg')\n","inference_dataset = GUM(INFERENCE_DATASET_DIR, transform=data_transforms, inference_mode=True)\n","inference_loader = DataLoader(inference_dataset, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, shuffle=False)\n","\n","# Initialize the Global Urban Mapper model for inference.\n","model = init_model(MODEL_CONFIG, CKPT_PATH, device=DEVICE)"],"metadata":{"id":"VZi1HVxnnXTq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Prediction"],"metadata":{"id":"yNUfjUvVoIqD"}},{"cell_type":"code","source":["model.eval()\n","with torch.no_grad():\n","    for inputs, img_metas in tqdm(inference_loader, desc='Inference Progress', unit='batch'):\n","\n","        inputs = inputs.to(DEVICE)\n","        outputs = model.test_step([inputs])\n","\n","        file_name = img_metas['filename'][0] # Assuming img_meta is a list of lists of dicts\n","        input_path = os.path.join(INFERENCE_DATASET_DIR, file_name)\n","        output_path = os.path.join(PREDICTED_DATASET_DIR, file_name)\n","        with rasterio.open(input_path) as src:\n","            profile = src.profile\n","\n","        # Adjust the profile to match the output dimensions and data type\n","        output = outputs[0].pred_sem_seg.data.cpu().numpy()\n","        profile.update(\n","            dtype=np.uint8,\n","            count=1  # Update the number of bands to the output's number of bands\n","        )\n","\n","        # Save the prediction\n","        if output.ndim == 2:\n","            output = output[np.newaxis, :, :]\n","\n","        # Write the output with the same profile as the input image\n","        with rasterio.open(output_path, 'w', **profile) as dst:\n","            dst.write(output)"],"metadata":{"id":"_qKz0X5Ba0zg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Hop9sWiW00sl"},"execution_count":null,"outputs":[]}]}